{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание модели и среды в Mujoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import scipy\n",
    "from SpiderEnv.SpiderEnv_many import SpiderEnv\n",
    "\n",
    "critic_learning_rate = 1e-3\n",
    "actor_learning_rate = 1e-4\n",
    "epochs_number = 3000000\n",
    "batch_size = 3000\n",
    "replay_buffer_size = batch_size\n",
    "\n",
    "discount_factor = 0.98\n",
    "lambda_factor = 0.96\n",
    "\n",
    "angle_normalization = 135\n",
    "\n",
    "\n",
    "env_name = 'SpiderEnv_many'\n",
    "\n",
    "algorithm_name = 'A2C-GAE'\n",
    "\n",
    "# Vectorized environment size\n",
    "environments_count = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    if env_name == 'SpiderEnv_many':\n",
    "        return SpiderEnv()\n",
    "    else:\n",
    "        return gym.make(env_name)\n",
    "\n",
    "\n",
    "env = create_environment()\n",
    "action_space = env.action_space.n if isinstance(env.action_space, gym.spaces.discrete.Discrete) else env.action_space.shape[0]\n",
    "observation_space = env.observation_space.n if isinstance(env.observation_space, gym.spaces.discrete.Discrete) else env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание архитектуры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-8\n",
    "\n",
    "def gaussian_loglikelihood(x, mu, log_std):\n",
    "    pre_sum = -0.5 * (((x - mu) / (tf.exp(log_std) + epsilon))**2 + 2 * log_std + np.log(2 * np.pi))\n",
    "    return tf.reduce_sum(pre_sum, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "class ActorNetworkContinuous:\n",
    "    def __init__(self):\n",
    "        self.state_ph = tf.placeholder(tf.float32, shape=[None, observation_space])\n",
    "\n",
    "        l1 = tf.layers.dense(self.state_ph, units=100, activation=tf.nn.tanh)\n",
    "        l2 = tf.layers.dense(l1, units=50, activation=tf.nn.tanh)\n",
    "        l3 = tf.layers.dense(l2, units=25, activation=tf.nn.tanh)\n",
    "        mu = tf.layers.dense(l3, units=action_space)\n",
    "\n",
    "        log_std = tf.get_variable(name='log_std', \n",
    "                                  initializer=-0.5 * np.ones(action_space, \n",
    "                                                             dtype=np.float32))\n",
    "        std = tf.exp(log_std)\n",
    "\n",
    "        self.action_op = mu + tf.random.normal(shape=tf.shape(mu)) * std\n",
    "\n",
    "        # Training\n",
    "        self.weight_ph = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.action_ph = tf.placeholder(shape=[None, action_space], dtype=tf.float32)\n",
    "\n",
    "        action_logprob = gaussian_loglikelihood(self.action_ph, mu, log_std)\n",
    "        self.loss = -tf.reduce_mean(action_logprob * self.weight_ph)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=actor_learning_rate)\n",
    "        self.update_op = optimizer.minimize(self.loss)\n",
    "\n",
    "\n",
    "class ActorNetworkDiscrete:\n",
    "    def __init__(self):\n",
    "        self.state_ph = tf.placeholder(tf.float32, shape=[None, observation_space])\n",
    "        l1 = tf.layers.dense(self.state_ph, units=20, activation=tf.nn.relu)\n",
    "        output_linear = tf.layers.dense(l1, units=action_space)\n",
    "\n",
    "        output = tf.nn.softmax(output_linear)\n",
    "        self.action_op = tf.squeeze(tf.multinomial(logits=output_linear,num_samples=1), \n",
    "                                    axis=1)\n",
    "\n",
    "        # Training\n",
    "        output_log = tf.nn.log_softmax(output_linear)\n",
    "\n",
    "        self.weight_ph = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.action_ph = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "\n",
    "        action_one_hot = tf.one_hot(self.action_ph, action_space)\n",
    "        responsible_output_log = tf.reduce_sum(output_log * action_one_hot, axis=1)\n",
    "        self.loss = -tf.reduce_mean(responsible_output_log * self.weight_ph)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=actor_learning_rate)\n",
    "        self.update_op = optimizer.minimize(self.loss)\n",
    "\n",
    "\n",
    "class CriticNetwork:\n",
    "    def __init__(self):\n",
    "        self.state_ph = tf.placeholder(tf.float32, shape=[None, observation_space])\n",
    "\n",
    "        l1 = tf.layers.dense(self.state_ph, units=100, activation=tf.nn.tanh)        \n",
    "        l2 = tf.layers.dense(l1, units=50, activation=tf.nn.tanh)\n",
    "        l3 = tf.layers.dense(l2, units=25, activation=tf.nn.tanh)\n",
    "        output = tf.layers.dense(l3, units=1)\n",
    "        \n",
    "        self.value_op = tf.squeeze(output, axis=-1)\n",
    "\n",
    "        # Training\n",
    "        self.value_ph = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        \n",
    "        self.loss = tf.losses.mean_squared_error(self.value_ph, self.value_op)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=critic_learning_rate)\n",
    "        self.update_op = optimizer.minimize(self.loss)\n",
    "\n",
    "actor = ActorNetworkContinuous()\n",
    "critic = CriticNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "sess.run(tf.local_variables_initializer())\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "\n",
    "def make_env(env_id, seed):\n",
    "    def _f():\n",
    "        env = create_environment()\n",
    "        env.reset()\n",
    "        \n",
    "        for i in range(int(200 * seed // environments_count)):\n",
    "            env.step(env.action_space.sample())\n",
    "        return env\n",
    "    return _f\n",
    "\n",
    "envs = [make_env(env_name, seed) for seed in range(environments_count)]\n",
    "\n",
    "envs = DummyVecEnv(envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генератор данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def discount_cumsum(x, coef):\n",
    "    return scipy.signal.lfilter([1], [1, float(-coef)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "discount_cumsum([1, 2, 4, 8], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_advantage(states, rewards):\n",
    "    values = sess.run(critic.value_op, feed_dict={critic.state_ph: states})\n",
    "    deltas = rewards - values\n",
    "    deltas = deltas + discount_factor * np.append(values[1:], np.array([0]))\n",
    "    \n",
    "    advantage = discount_cumsum(deltas, coef=lambda_factor * discount_factor)\n",
    "    return advantage, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(envs, batch_size, replay_buffer_size):\n",
    "    envs_number = envs.num_envs\n",
    "    observations = [[0 for i in range(observation_space)] for i in range(envs_number)]\n",
    "    \n",
    "    replay_buffer = np.empty((0,4), np.float32)\n",
    "    \n",
    "    rollouts = [np.empty((0, 3)) for i in range(envs_number)]\n",
    "\n",
    "    while True:\n",
    "        history = {'reward': [], 'max_action': [], \n",
    "                   'mean_advantage': [], 'mean_value': []}\n",
    "        replay_buffer = replay_buffer[batch_size:]\n",
    "        \n",
    "        # Main sampling cycle\n",
    "        while len(replay_buffer) < replay_buffer_size:\n",
    "            actions = sess.run(actor.action_op, \n",
    "                               feed_dict={actor.state_ph: observations})\n",
    "            observations_old = observations\n",
    "            observations, rewards, dones, _ = envs.step(actions * angle_normalization)\n",
    "            observations /= angle_normalization\n",
    "            \n",
    "            history['max_action'].append(np.abs(actions).max())\n",
    "            \n",
    "            time_point = np.array(list(zip(observations_old, actions, rewards)))\n",
    "            for i in range(envs_number):\n",
    "\n",
    "                rollouts[i] = np.append(rollouts[i], [time_point[i]], axis=0) \n",
    "            \n",
    "\n",
    "            if dones.all():\n",
    "                print('WARNING: envs are in sync!! This makes sampling inefficient!')\n",
    "\n",
    "            done_indexes = np.arange(envs_number)[dones]\n",
    "            for i in done_indexes:\n",
    "                rewards_trajectory = rollouts[i][:, 2].copy()\n",
    "                history['reward'].append(rewards_trajectory.sum())\n",
    "                \n",
    "\n",
    "                advantage, values = estimate_advantage(states=np.array(rollouts[i][:, 0].tolist()),\n",
    "                                                       rewards=rewards_trajectory)\n",
    "                \n",
    "                history['mean_value'].append(values.mean())\n",
    "                history['mean_advantage'].append(advantage.mean())\n",
    "\n",
    "                rollouts[i][:, 2] = advantage\n",
    "\n",
    "                discounted_reward_to_go = discount_cumsum(rewards_trajectory, \n",
    "                                                          coef=discount_factor)\n",
    "\n",
    "                rollout = np.hstack((rollouts[i], \n",
    "                                     np.expand_dims(discounted_reward_to_go, axis=-1)))                \n",
    "                replay_buffer = np.append(replay_buffer, rollout, axis=0)\n",
    "                rollouts[i] = np.empty((0, 3))\n",
    "        \n",
    "\n",
    "        np.random.shuffle(replay_buffer)\n",
    "        \n",
    "\n",
    "        replay_buffer = replay_buffer[:replay_buffer_size]\n",
    "        yield replay_buffer[:batch_size], history\n",
    "\n",
    "\n",
    "a = generate_batch(envs, 8, 64)\n",
    "\n",
    "for i in range(10):\n",
    "    next(a)\n",
    "next(a)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тренировка агента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'reward': [], 'actor_loss': [], 'critic_loss': [], \n",
    "           'max_action': [], 'mean_value': [], 'mean_advantage': []}\n",
    "max_value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "\n",
    "batch_generator = generate_batch(envs,\n",
    "                                 batch_size=batch_size,\n",
    "                                 replay_buffer_size=replay_buffer_size)\n",
    "\n",
    "\n",
    "print('Charging generators')\n",
    "for i in range(20):\n",
    "    next(batch_generator)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "for epoch in tqdm_notebook(range(epochs_number)):\n",
    "    batch, batch_history = next(batch_generator)\n",
    "    history['reward'] += batch_history['reward']\n",
    "    history['max_action'] += batch_history['max_action']\n",
    "    history['mean_advantage'] += batch_history['mean_advantage']\n",
    "    history['mean_value'] += batch_history['mean_value']\n",
    "\n",
    "\n",
    "    value = int(np.mean(history[\"reward\"][-10:]))\n",
    "    if max_value < value:\n",
    "        save_path = saver.save(sess, f'./models/{env_name}-{algorithm_name}-reward({value}).ckpt')\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "        max_value = value\n",
    "        \n",
    "    if epoch % 1000 == 0:\n",
    "        with open(f'models/{env_name}-{algorithm_name}-reward({value}).history', 'wb') as f:\n",
    "            pickle.dump(history, f)\n",
    "            \n",
    "\n",
    "    # Train actor\n",
    "    _, actor_loss = sess.run([actor.update_op, actor.loss], \n",
    "                             feed_dict={actor.state_ph: np.array(batch[:, 0].tolist()),\n",
    "                                        actor.action_ph: np.array(batch[:, 1].tolist()),\n",
    "                                        actor.weight_ph: batch[:, 2]})\n",
    "    # Train critic\n",
    "    for j in range(10):\n",
    "        _, critic_loss = sess.run([critic.update_op, critic.loss], \n",
    "                                  feed_dict={critic.state_ph: np.array(batch[:, 0].tolist()),\n",
    "                                             critic.value_ph: batch[:, 3]})\n",
    "    \n",
    "    history['critic_loss'].append(critic_loss)\n",
    "    history['actor_loss'].append(actor_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "observations = env.reset()\n",
    "rewards_sum = 0\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    actions = sess.run(actor.action_op, feed_dict={actor.state_ph: [observations]})\n",
    "\n",
    "    observations_old = observations\n",
    "    observations, rewards, dones, _ = env.step(actions[0] * angle_normalization)\n",
    "    observations = observations.astype(np.float32) / angle_normalization\n",
    "    rewards_sum += rewards\n",
    "\n",
    "    if dones:\n",
    "        observations = env.reset()\n",
    "        print('Done', rewards_sum)\n",
    "        rewards_sum = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
